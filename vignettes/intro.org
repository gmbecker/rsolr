#+TITLE: Introduction to rsolr

* Introduction
  The =rsolr= package provides an idiomatic (R-like) and extensible
  interface between R and Solr, a search engine and database. Like an
  onion, the interface consists of several layers, along a gradient of
  abstraction, so that simple problems are solved simply, while more
  complex problems may require some peeling and perhaps tears. The
  interface is idiomatic, syntactically but also in terms of
  /intent/. While Solr provides a search-oriented interface, we
  recognize it as a document-oriented database. While not entirely
  schemaless, its schema is extremely flexible, which makes Solr an
  effective database for prototyping and adhoc analysis. R is designed
  for manipulating data, so =rsolr= maps common R data manipulation
  verbs to the Solr database and its (limited) support for
  analytics. In other words, =rsolr= is for analysis, not search,
  which has presented some fun challenges in design. Hopefully it is
  useful --- we had not tried it until writing this document.

  We have interfaced with all of the Solr features that are relevant
  to data analysis, with the aim of implementing many of the
  fundamental data munging operations. Those operations are listed in
  the table below, along with how we have mapped those operations to
  existing and well-known functions in the base R API. When called on
  =rsolr= data structures, those functions should behave analogously
  to the existing implementations for =data.frame=. Note that more
  complex operations, such as joining and reshaping tables, are best
  left to more sophisticated frameworks. After all, Solr is a search
  engine. Give it a break.

  | Operation      | R function  |
  |----------------+-------------|
  | Filtering      | =subset=    |
  | Transformation | =transform= |
  | Sorting        | =sort=      |
  | Aggregation    | =aggregate= |
  
* Demonstration
** The Dataset
   As part demonstration and part proof of concept, we will attempt to
   follow the introductory workflow from the =dplyr= vignette. The
   dataset describes all of the airline flights departing New York City
   in 2013. It is provided by the =nycflights13= package, so please see
   its documentation for more details.
   #+begin_src R
     library(nycflights13)
     dim(flights)
     head(flights)
   #+end_src

** Populating a Solr core
   
   The first step is getting the data into a Solr /core/, which is
   what Solr calls a database. This involves writing a schema in XML,
   installing and configuring Solr, launching the server, and
   populating the core with the actual data. Our expectation is that
   most use cases of =rsolr= will involve accessing an existing,
   centrally deployed, usually read-only Solr instance, so those are
   typically not major concerns. However, to conveniently demonstrate
   the software, we need to violate all of those assumptions.
   Luckily, we have managed to embed an example Solr installation
   within =rsolr=. We also provide a mechanism for autogenerating a
   Solr schema from a =data.frame=. This could be useful in practice
   for producing a template schema that can be tweaked and deployed in
   shared Solr installations. Taken together, the process turns out to
   not be very intimidating.

   We begin by generating the schema and starting the demo Solr
   instance. Note that this instance is really only meant for
   demonstrations. You should not abuse it like the people abused the
   poor built-in R HTTP daemon.
   #+begin_src R
     library(rsolr)
     schema <- deriveSolrSchema(flights)
     solr <- rsolr:::TestSolr(schema)
   #+end_src
   
   Next, we need to populate the core with our data. This requires a
   way to interact with the core from R. =rsolr= provides direct
   access to cores, as well as two high-level interfaces that
   represent a dataset derived from a core (rather than the core
   itself). The two interfaces each correspond to a particular shape
   of data. /SolrList/ behaves like a list, while /SolrFrame/ behaves
   like a table (data frame). /SolrList/ is useful for when the data
   are ragged, as is often the case for data stored in Solr. The Solr
   schema is so dynamic that we could trivially define a schema with a
   virtually infinite number of fields, and each document could have
   its own unique set of fields. However, since our data are tabular,
   we will use /SolrFrame/ for this exercise.
   #+begin_src R
   sr <- SolrFrame(solr$uri)
   #+end_src
   Finally, we load our data into the Solr dataset:
   #+begin_src R
   sr[] <- flights
   #+end_src
   This takes a while, since Solr has to generate all sorts of
   indices, etc.

   As /SolrFrame/ behaves much like a base R data frame, we can
   retrieve the dimensions and look at the head of the dataset:
   #+begin_src R
     dim(sr)
     head(sr)
   #+end_src
   The =head()= method returns virtually instantaneously, because the
   query is executed lazily, whenever the data are requested. One
   example of a request is when we print the object, as above.

   Comparing the output above the that of the earlier call to
   =head(flights)= reveals that the data are virtually identical. As
   Solr is just a search engine (on steroids), a significant amount of
   engineering was required to achieve that result.
   
** Restricting by row
   The simplest operation is filtering the data, i.e., restricting it
   to a subset of interest. Even a search engine should be good at
   that. Below, we use =subset= to restrict to the flights to those
   departing on January 1 (2013).
   #+begin_src R
     subset(sr, month == 1 & day == 1)
   #+end_src
   Note how the records at the bottom contain missing values. Solr
   does not provide any facilities for missing value representation,
   but we mimic it by excluding those fields from those documents.

   We can also extract ranges of data using the canonical =window()=
   function:
   #+begin_src R
     window(sr, start=1L, end=10L)
   #+end_src
   Or, as we have already seen, the more convenient:
   #+begin_src R
     head(sr, 10L)
   #+end_src
   It is unfortunately not feasible to randomly access Solr records by
   index, because numeric indexing is a foreign concept to a search
   engine. Solr does however support retrieval by a key that has a
   unique value for each document. These data lack such a key, but it
   is easy to add one and indicate as such to =deriveSolrSchema()=.

** Sorting
   To sort the data, we just call =sort()= and describe the order by
   passing a formula via the =by= argument. For example, we sort by
   year, breaking ties with month, then day:
   #+begin_src R
   sort(sr, by = ~ year + month + day)
   #+end_src

   To sort in decreasing order, just pass =decreasing=TRUE= as usual:
   #+begin_src R
   sort(sr, by = ~ arr_delay, decreasing=TRUE)
   #+end_src

** Restricting by field
   Just as we can use =subset= to restrict by row, we can also use it
   to restrict by column:
   #+begin_src R
   subset(sr, select=c(year, month, day))
   #+end_src
   The =select= argument is analogous to that of =subset.data.frame=:
   it is evaluated to set of field names to which the dataset is
   restricted. The above example is static, so it is equivalent to:
   #+begin_src R
   sr[c("year", "month", "day")]
   #+end_src
   
   But with =subset= we can also specify dynamic expressions,
   including ranges:
   #+begin_src R
   subset(sr, select=year:day)
   #+end_src
   And exclusion:
   #+begin_src R
   subset(sr, select=-(year:day))
   #+end_src
   
   Solr also has native support for globs:
   #+begin_src R
   sr[c("arr_*", "dep_*")]
   #+end_src

   While we are dealing with fields, we should mention that renaming
   is also possible:
   #+begin_src R
### FIXME: broken in current Solr CSV writer
   rename(sr, tail_num = "tailnum")
   #+end_src
   
** Transformation
   To compute new columns from existing ones, we can, as usual, call
   the =transform= function:
   #+begin_src R
     transform(sr,
               gain = arr_delay - dep_delay,
               speed = distance / air_time * 60)
   #+end_src
   
** Summarization
   Data summarization is about reducing large, complex data to
   smaller, simpler data that we can understand.

   A common type of summarization is aggregation, which is typically
   defined as a three step process:
   1. Split the data into groups, usually by the the interaction of
      some factor set,
   2. Summarize each group to a single value,
   3. Combine the summaries.

   Solr supports the following types of data aggregation:
   * Mean,
   * Min/max,
   * Variance and standard deviation,
   * Sum,
   * Count (tabulation).
   Notably missing is the median, but we should expect that soon, as
   Solr is evolving very quickly in this area. One obstacle is likely
   that the median is notoriously difficult to distribute.

   A prerequisite of aggregation is finding the distinct field
   combinations that correspond to each correspond to a group. Those
   combinations themselves constitute a useful summary, and we can
   retrieve them with =unique=:
   #+begin_src R
   unique(sr["tailnum"])
   unique(sr[c("origin", "tailnum")])
   #+end_src
   
   Solr also supports extracting the top or bottom N documents, after
   ranking by some field, optionally by group.
